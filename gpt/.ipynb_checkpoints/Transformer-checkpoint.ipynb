{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d1ee96-748b-4a22-86fa-87354501c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from math import floor\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eefc29c-2685-42b5-96c8-f1a060ae7038",
   "metadata": {},
   "source": [
    "# Simple end-to-end plan\n",
    "\n",
    "1. **Load data**  \n",
    "   * Read `enwik9` as bytes, keep ints 0-255.  \n",
    "   * Split once into train / validation.\n",
    "\n",
    "2. **Build dataset**  \n",
    "   * Each sample: `ctx_len+1` consecutive bytes.  \n",
    "   * Inputs = first `ctx_len`; targets = bytes 1…`ctx_len`.  \n",
    "   * Wrap in `DataLoader(batch_size=64, shuffle=True)`.\n",
    "\n",
    "3. **Set up model**  \n",
    "   * Use your `CTransformer` (d_model = 256, ctx_len = 256, mask=True).  \n",
    "   * Move to `device = torch.device(\"cuda\")`.\n",
    "\n",
    "4. **Configure training**  \n",
    "   * Loss: `nn.CrossEntropyLoss()`.  \n",
    "   * Optimiser: `AdamW(lr=3e-4, weight_decay=1e-4)`.  \n",
    "   * Use mixed precision (`torch.cuda.amp`) and clip grads at 1.0.  \n",
    "   * Scheduler: warm-up 200 steps → cosine decay.\n",
    "\n",
    "5. **Training loop**  \n",
    "   * For each batch: forward → loss → backward → step → scheduler.  \n",
    "   * Log train loss every 100 steps.\n",
    "\n",
    "6. **Validate & checkpoint**  \n",
    "   * Run no-grad pass on validation set each epoch.  \n",
    "   * Save `state_dict` when val loss improves.\n",
    "\n",
    "7. **Quick sampling test**  \n",
    "   * In `eval` mode, feed a prompt, soft-max logits, sample next byte, repeat.  \n",
    "   * If output starts to look like English, training is on track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727b44d-2fd7-44d5-8381-5f86b54654e3",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0a07ed-8808-43e1-9296-97ac3f163d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=4, mask=False):\n",
    "        super().__init__()\n",
    "        assert k % heads == 0\n",
    "        self.mask = mask\n",
    "        self.k, self.heads = k, heads\n",
    "\n",
    "        self.tokeys = nn.Linear(k, k, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k, bias=False)\n",
    "        self.tovalues = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b = batch size\n",
    "        # t = context window size\n",
    "        # k = hidden dimension\n",
    "        # h = # of heads\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        keys = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values = self.tovalues(x)\n",
    "\n",
    "        # s = dimension per head\n",
    "        s = k // h\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "\n",
    "        # transpose 1,2 so that we can compute along every batch/head\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b*h, t, s)\n",
    "\n",
    "        # (t, s) x (s, t) -> (t, t)\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        # Mask to prevent looking into the future\n",
    "        if self.mask:\n",
    "            indices = torch.triu_indices(t, t, offset=1, device=dot.device)\n",
    "            dot[:, indices[0], indices[1]] = float('-inf')\n",
    "            \n",
    "        dot = dot / (s ** (1/2))\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # (t, t) x (t, s) -> (t, s)\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s*h)\n",
    "\n",
    "        return self.unifyheads(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a30ca72-c07b-44a5-8d93-78b16d3df056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, heads=heads, mask=True) # wasted a day training with mask=False lol\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(k, 4*k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*k, k)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bffff00-43ee-4d09-9520-1ad2d1d1b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=256, ctx_len=256, d_model=256, n_layers=12, n_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # token & position embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(ctx_len, d_model))\n",
    "\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        # output embedding\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, idx):\n",
    "        '''\n",
    "        idx: (batch, seq_len) tensor of character IDs\n",
    "        returns logits: (batch, seq_len, vocab_size)\n",
    "        '''\n",
    "        b, t = idx.shape\n",
    "        x = self.tok_emb(idx) + self.pos_emb[:t]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a713b5-5446-4aee-8af4-a6d532e325a6",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb51d855-dd53-4aca-a1ed-ec00dc8083c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: np.ndarray, ctx_len: int):\n",
    "        '''\n",
    "        data: 1-D uint8 array of bytes (int 0-255)\n",
    "        ctx_len: sequence length\n",
    "        Each item returns (input_idxs, target_idxs), both of length ctx_len\n",
    "        '''\n",
    "        self.data = torch.from_numpy(data) #data.astype(np.int64))\n",
    "        self.ctx_len = ctx_len\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Tells torch.DataLoader how many samples there are.\n",
    "        '''\n",
    "        return len(self.data) - self.ctx_len\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        # Shuffle in here. \n",
    "        # DataLoader(..., shuffle=True) breaks the notebook for some reason. I think it might be a memory issue. \n",
    "        # This will also break multiple workers, so be careful. \n",
    "        i = torch.randint(0, len(self.data) - self.ctx_len - 1, (1,)).item()\n",
    "        \n",
    "        chunk = self.data[i: i + self.ctx_len + 1]\n",
    "        return chunk[:-1].long(), chunk[1:].long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3547e3f-34ac-4e00-ac83-0a50915a8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/c/ad_astra/data/enwik9/enwik9'\n",
    "val_frac = 0.01\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "shuffle = False\n",
    "\n",
    "ctx_len = 256\n",
    "batch_size = 192\n",
    "vocab_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3af833-02cd-4af5-8f20-d5cfda631043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = Path(data_path).read_bytes()\n",
    "raw = np.memmap(data_path, dtype=np.uint8, mode='r')\n",
    "\n",
    "data = np.frombuffer(raw, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a9122a-d650-49f6-9126-02b4c2603f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19639/2843144751.py:8: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  self.data = torch.from_numpy(data) #data.astype(np.int64))\n"
     ]
    }
   ],
   "source": [
    "split = len(data) - floor(len(data) * val_frac)\n",
    "train_data, val_data = data[:split], data[split:]\n",
    "train_ds, val_ds = ByteDataset(train_data, ctx_len), ByteDataset(val_data, ctx_len)\n",
    "\n",
    "# Jimmy Notes\n",
    "# pin_memory: page locks data so DMA can stream directly to GPU\n",
    "# drop_last: drops final batch if dataset size isn't divisible by batch_size so every sample is the same size\n",
    "train_loader = iter(torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    pin_memory=pin_memory,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    "))\n",
    "\n",
    "val_loader = iter(torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    pin_memory=pin_memory,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a9a13-31a7-4c25-8079-4b66ddc09362",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319746ad-e7fe-40bc-a2a0-8faac2d09790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, max_steps, train=True):\n",
    "    total, steps = 0, 0\n",
    "    model.train(train)\n",
    "    torch.set_grad_enabled(train)\n",
    "    loader_len = len(loader)\n",
    "    train_loss_hist = []\n",
    "    while steps < max_steps:\n",
    "        t0 = time.time()\n",
    "        xb, yb = next(loader)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        data_ms = (time.time() - t0) * 1e3\n",
    "\n",
    "        assert xb.is_cuda and next(model.parameters()).is_cuda, \"data or model not on GPU!\"\n",
    "        fwd_start = time.time()\n",
    "        with autocast():\n",
    "            # Runs a forward pass. model(xb) (instead of model.forward(xb))\n",
    "            # runs a pre/post hooks. \n",
    "            # Returns shape (batch, ctx_len, vocab_size)\n",
    "            logits = model(xb)\n",
    "\n",
    "            # N = batch_size * ctx_len\n",
    "            # logits.view: (N, vocab_size)\n",
    "            # yb.view: (N, )\n",
    "            # CrossEntropyLoss automatically interprets yb as the index of the correct label.\n",
    "            loss = criterion(logits.view(-1, vocab_size), yb.view(-1))\n",
    "        fwd_ms = (time.time() - fwd_start) * 1e3\n",
    "\n",
    "        bwd_start = time.time()\n",
    "        if train:\n",
    "            # Scales loss (to avoid underflow), then calls backward\n",
    "            scaler.scale(loss).backward()\n",
    "            # optimizer holds references to the Parameters inside our computation graph. \n",
    "            # unscale_ goes through these references and unscales the gradients.\n",
    "            scaler.unscale_(optimizer)\n",
    "            # simple clipping\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update the weights\n",
    "            scaler.step(optimizer)\n",
    "            # Update the state of the scaler. Specifically slowly raises it if found_inf=0\n",
    "            # and halves it if found_inf=1\n",
    "            # The approach is to raise it as much as we can without overflowing to have more float dynamic range\n",
    "            scaler.update()\n",
    "            # Go through every registered parameter and clear the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # Advanced learning rate scheduler\n",
    "            scheduler.step()\n",
    "        bwd_ms = (time.time() - bwd_start) * 1e3\n",
    "        \n",
    "        total += loss.item()\n",
    "        steps += 1\n",
    "        train_loss_hist.append(total / steps)\n",
    "        if train and steps % 1_000 == 0:\n",
    "            print(\n",
    "                f\"step {steps:>8,}  |  \"\n",
    "                f\"data {data_ms:6.1f} ms  \"\n",
    "                f\"fwd {fwd_ms:6.1f} ms  \"\n",
    "                f\"bwd+opt {bwd_ms:6.1f} ms  \"\n",
    "                f\"train‑loss {total/steps:.4f}\"\n",
    "            )\n",
    "            # print(f'step {steps:>8,} / {max_steps:>8,}  |  train-loss {total/steps:.4f}')\n",
    "    return train_loss_hist\n",
    "\n",
    "def sample(model, \n",
    "           prompt: List[int], \n",
    "           n_tokens=256, \n",
    "           temperature=1.0\n",
    "          ):\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    # Convert prompt into LongTensor and add another dimension to the beginning (batch_size=1) basically\n",
    "    idx = torch.tensor(prompt, dtype=torch.long, device=device)[None]\n",
    "    # Don't store intermediate results because we will not backprop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            # Grab logits for the last position and apply temperature\n",
    "            logits = model(idx)[:, -1] / temperature\n",
    "            # Sample from multinomial distribution\n",
    "            next_id = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "            # Append to running generation\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "    # Convert results back to bytes\n",
    "    return bytes(idx.squeeze().tolist())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf660673-a008-4a48-8885-8fecc2599f4e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5844c741-8e9f-40d3-ba92-98577e35fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = 'cuda'\n",
    "model_gen = lambda: CTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    ctx_len=ctx_len,\n",
    "    d_model=256,\n",
    "    n_layers=12,\n",
    "    n_heads=8\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3357faa-33e9-4d05-a646-f95b6b80be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19639/1470023083.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, or load checkpoint\n",
    "model = model_gen()\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "tokens_seen = 0\n",
    "\n",
    "# loading block\n",
    "load_path = 'models/ctransformer_enwik9_2025-07-27_232950.pt'\n",
    "checkpoint = torch.load(load_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['weights'])\n",
    "train_loss_hist = checkpoint['train_loss_hist']\n",
    "val_loss_hist = checkpoint['val_loss_hist']\n",
    "tokens_seen = checkpoint['tokens_seen']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30_000, eta_min=1e-5)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11161184-fc0b-4756-86b4-5a44d075f16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77b9f663-13d5-40a7-9ecb-8cda3d1049f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7372800000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['tokens_seen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2a1e30-caaf-46b1-a3c0-d563b5dc2d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.67481427, 2.55015864, 2.50768377, 2.52870738, 2.55466045,\n",
       "       2.5462475 , 2.49911525, 2.42671841, 2.39561883, 2.41323156,\n",
       "       2.46038368, 2.47579492, 2.44270166, 2.38764034, 2.35806139])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b0e02-cb82-4669-b384-434030bacda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "best_val = float('inf')\n",
    "\n",
    "train_steps_per_cycle = 10_000\n",
    "val_steps_per_cycle = 1_000\n",
    "tokens_seen = 0\n",
    "\n",
    "try:\n",
    "    for cycle in range(5_000):\n",
    "        print(f'Cycle {cycle:>6}')\n",
    "        train_loss_hist += run_epoch(model, train_loader, train_steps_per_cycle, train=True)\n",
    "        train_loss = train_loss_hist[-1]\n",
    "        tokens_seen += batch_size * ctx_len * train_steps_per_cycle\n",
    "        \n",
    "        val_loss = run_epoch(model, val_loader, val_steps_per_cycle, train=False)[-1] \n",
    "        val_loss_hist.append(val_loss)\n",
    "        print(f'cycle {cycle:03d} train {train_loss:.4f} val {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss \n",
    "      \n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "            save_path = f'models/ctransformer_enwik9_{timestamp}.pt'\n",
    "            torch.save({\n",
    "                'train_loss_hist': train_loss_hist,\n",
    "                'val_loss_hist': val_loss_hist, \n",
    "                'tokens_seen': tokens_seen,\n",
    "                'weights': model.state_dict()\n",
    "            }, save_path)\n",
    "            print(f'checkpoint saved: {save_path}')\n",
    "            print(f'{val_loss_hist=}')\n",
    "            print(sample(model, list(b\"The \"), 200).decode(\"ascii\", \"ignore\"))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Save checkpoint\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    save_path = f'models/ctransformer_enwik9_interrupt_{timestamp}.pt'\n",
    "    torch.save({\n",
    "        'train_loss_hist': train_loss_hist,\n",
    "        'val_loss_hist': val_loss_hist, \n",
    "        'tokens_seen': tokens_seen,\n",
    "        'weights': model.state_dict()\n",
    "    }, save_path)\n",
    "    \n",
    "    # Save loss histories\n",
    "    with open('val_loss_hist.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(map(str, val_loss_hist)))\n",
    "    with open('train_loss_hist.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(map(str, train_loss_hist)))\n",
    "        \n",
    "    print(f'\\nInterrupted — checkpoint saved to {save_path}')\n",
    "    raise  # re‑raise so Jupyter stops execution\n",
    "\n",
    "except Exception as e:\n",
    "    # save loss curve then raise again lol\n",
    "    with open('val_loss_hist.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(map(str, val_loss_hist)))\n",
    "    with open('train_loss_hist.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(map(str, train_loss_hist)))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fca58-99fe-4cff-8395-1d514939c973",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97974c6e-9e11-4ecd-ab91-04773175f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is a &quot;distorted biography&quot; dispute that includes [[SoundExperience]] and [[Ashburner's Institute]] obsolete.\n",
      "\n",
      "A 2001 soundtrack, the star in [[London]] appears on-dip-script, briefly turning S\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, list(b\"OpenAI is\"), 200).decode(\"ascii\", \"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757ead89-d1cb-445f-a775-8e5e5f392422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional film school founded by Mabbuja Steel in April [[2002]]. The large motets Jacksonville had it to be  read in [[Lonely River (Manila, California)|Lonely River]]. The Leadership motets Jacksonvilli as dedicated to t\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, list(b\"Traditional film school \"), 200).decode(\"ascii\", \"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9acdc-2bf0-41de-a8e0-b252536b543a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40ac67-177a-4925-a9cf-5795de50b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463e1fa-9d62-4b0c-95f5-b4db282a8544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ac5cc-ed3f-4e97-93be-fe71628d196b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
